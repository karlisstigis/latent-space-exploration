{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df1d93a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0. Install dependencies (run once per environment)\n",
    "\n",
    "%pip install fastai scikit-learn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f287ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and configuration\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from fastai.basics import *\n",
    "import umap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, ParameterSampler, cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from time import perf_counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Input datasets (normalized 0..1)\n",
    "GENERATED_CSV_PATH = Path(\"Palettes/Generated/palette_export_generated.csv\")\n",
    "ADOBE_CSV_PATH = Path(\"Palettes/Adobe/adobe_palettes.csv\")\n",
    "\n",
    "# Rules-only encoder model (trained on generated data only)\n",
    "MODEL_PATH = Path(\"trained_models/palette_autoencoder_rules_only.pkl\")\n",
    "\n",
    "# Permutation invariance\n",
    "CLASS_TRAIN_PERM_AUGMENT = True\n",
    "CLASS_TRAIN_PERM_K = 8   # additional random perms per palette during classifier training\n",
    "CLASS_INFER_PERM_AVG = True\n",
    "CLASS_INFER_PERM_K = 20  # additional random perms per palette during inference averaging\n",
    "CLASS_PERM_SEED = 42\n",
    "\n",
    "# Prototype confidence scaling\n",
    "PROTO_CONF_SCALE = 10.0\n",
    "\n",
    "# UMAP context\n",
    "INCLUDE_GENERATED_IN_UMAP = True\n",
    "\n",
    "# Outputs\n",
    "OUT_UMAP_PRED_PATH = Path(\"out/adobe_umap_law_predictions.csv\")\n",
    "OUT_POPULARITY_PATH = Path(\"out/adobe_rule_popularity.csv\")\n",
    "OUT_METRICS_PATH = Path(\"out/generated_classifier_metrics.csv\")\n",
    "\n",
    "COLOR_COUNT = 5\n",
    "SRGB_TO_LINEAR = True  # convert Adobe sRGB to linear before encoding\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Random Forest auto-tuning (ranges)\n",
    "RF_AUTO_TUNE = True\n",
    "RF_RANDOM_STATE = 42\n",
    "RF_SEARCH_N_ITER = 24\n",
    "RF_CV_FOLDS = 4\n",
    "RF_SCORING = \"accuracy\"\n",
    "RF_FIT_N_JOBS = 31  # use 31/32 threads, leave one for the system\n",
    "RF_CV_N_JOBS = 1    # avoid nested parallelism\n",
    "\n",
    "# Probability calibration for better confidence quality\n",
    "RF_USE_PROB_CALIBRATION = True\n",
    "RF_CALIBRATION_METHOD = \"sigmoid\"  # sigmoid or isotonic\n",
    "RF_CALIBRATION_SPLIT = 0.15\n",
    "\n",
    "RF_PARAM_GRID = {\n",
    "    \"n_estimators\": [200, 400, 600, 800, 1000, 1200],\n",
    "    \"max_depth\": [None, 12, 20, 30, 40],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 5],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n",
    "    \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e3b25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define model class (required for load_learner)\n",
    "\n",
    "class PaletteAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4807bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Utilities (data loading + encoding + permutations)\n",
    "\n",
    "feature_cols = [f\"{x}{i}\" for i in range(1, COLOR_COUNT + 1) for x in (\"r\", \"g\", \"b\")]\n",
    "expected_generated = [\"law\", \"id_palette\"] + [\n",
    "    f\"c{x}{i}\" for i in range(1, COLOR_COUNT + 1) for x in (\"r\", \"g\", \"b\")\n",
    "]\n",
    "expected_adobe = [f\"{x}{i}\" for i in range(1, COLOR_COUNT + 1) for x in (\"r\", \"g\", \"b\")]\n",
    "\n",
    "\n",
    "def _srgb_to_linear(arr):\n",
    "    arr = np.clip(arr, 0.0, 1.0)\n",
    "    return np.where(arr <= 0.04045, arr / 12.92, ((arr + 0.055) / 1.055) ** 2.4)\n",
    "\n",
    "\n",
    "def _validate_normalized(df, cols, source_name):\n",
    "    min_v = float(df[cols].min().min())\n",
    "    max_v = float(df[cols].max().max())\n",
    "    if min_v < 0.0 or max_v > 1.0:\n",
    "        raise ValueError(\n",
    "            f\"{source_name} contains non-normalized values (min={min_v:.4f}, max={max_v:.4f}). \"\n",
    "            \"Expected all color channels in [0, 1].\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_generated_df(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Generated CSV not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    missing = [c for c in expected_generated if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Generated column mismatch for {path}. Missing {missing}, got {list(df.columns)}\")\n",
    "\n",
    "    df = df.drop(columns=[c for c in (\"palette_name\", \"batch\") if c in df.columns])\n",
    "    rename_map = {f\"c{x}{i}\": f\"{x}{i}\" for i in range(1, COLOR_COUNT + 1) for x in (\"r\", \"g\", \"b\")}\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    _validate_normalized(df, feature_cols, f\"Generated CSV ({path})\")\n",
    "    df[feature_cols] = df[feature_cols].astype(\"float32\")\n",
    "    df[[\"law\", \"id_palette\"]] = df[[\"law\", \"id_palette\"]].astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_adobe_df(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Adobe CSV not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    if list(df.columns) != expected_adobe:\n",
    "        raise ValueError(f\"Adobe column mismatch for {path}. Expected {expected_adobe}, got {list(df.columns)}\")\n",
    "\n",
    "    df.insert(0, \"id_palette\", -1)\n",
    "    df.insert(0, \"law\", -1)\n",
    "\n",
    "    _validate_normalized(df, feature_cols, f\"Adobe CSV ({path})\")\n",
    "    if SRGB_TO_LINEAR:\n",
    "        df[feature_cols] = _srgb_to_linear(df[feature_cols].to_numpy(dtype=\"float32\"))\n",
    "\n",
    "    df[feature_cols] = df[feature_cols].astype(\"float32\")\n",
    "    df[[\"law\", \"id_palette\"]] = df[[\"law\", \"id_palette\"]].astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_encoder(model_path: Path):\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}. Run the training notebook first.\")\n",
    "    learner = load_learner(model_path)\n",
    "    learner.model.eval()\n",
    "    return learner.model\n",
    "\n",
    "\n",
    "def encode_latents(model, data, batch_size=256):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    zs = []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(data), batch_size):\n",
    "            xb = torch.from_numpy(data[start:start + batch_size]).to(device)\n",
    "            zs.append(model.encoder(xb).cpu())\n",
    "    return torch.cat(zs).numpy()\n",
    "\n",
    "\n",
    "def apply_palette_permutation(x_rgb15: np.ndarray, perm: np.ndarray) -> np.ndarray:\n",
    "    x = x_rgb15.reshape(-1, COLOR_COUNT, 3)\n",
    "    x = x[:, perm, :]\n",
    "    return x.reshape(-1, COLOR_COUNT * 3)\n",
    "\n",
    "\n",
    "def augment_training_permutations(x_rgb15: np.ndarray, y: np.ndarray, k: int, seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    xs = [x_rgb15]\n",
    "    ys = [y]\n",
    "    for _ in range(k):\n",
    "        perm = rng.permutation(COLOR_COUNT)\n",
    "        xs.append(apply_palette_permutation(x_rgb15, perm))\n",
    "        ys.append(y)\n",
    "    return np.concatenate(xs, axis=0), np.concatenate(ys, axis=0)\n",
    "\n",
    "\n",
    "def softmax_from_neg_dist(dist: np.ndarray, scale: float):\n",
    "    scores = np.exp(-scale * dist)\n",
    "    return scores / scores.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "271a2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated training source: Palettes\\Generated\\palette_export_generated.csv\n",
      "Adobe inference source: Palettes\\Adobe\\adobe_palettes.csv\n",
      "Encoder model: trained_models\\palette_autoencoder_rules_only.pkl\n",
      "Generated base rows: 7000\n",
      "Generated train rows (after perm aug): 63000\n",
      "Adobe rows: 1000\n",
      "Latent train shape: (63000, 16)\n",
      "Latent base shape: (7000, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OHM\\Semester_3\\fast_ai_study\\.venv\\Lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# 4. Build train/eval sets (generated only) + encode with rules-only model\n",
    "\n",
    "generated_path = GENERATED_CSV_PATH\n",
    "adobe_path = ADOBE_CSV_PATH\n",
    "\n",
    "print(\"Generated training source:\", generated_path)\n",
    "print(\"Adobe inference source:\", adobe_path)\n",
    "print(\"Encoder model:\", MODEL_PATH)\n",
    "\n",
    "gen_df = load_generated_df(generated_path)\n",
    "adobe_df = load_adobe_df(adobe_path)\n",
    "\n",
    "X_gen_base = gen_df[feature_cols].to_numpy(dtype=\"float32\")\n",
    "y_gen_base = gen_df[\"law\"].to_numpy(dtype=\"int64\")\n",
    "X_adobe_base = adobe_df[feature_cols].to_numpy(dtype=\"float32\")\n",
    "\n",
    "if CLASS_TRAIN_PERM_AUGMENT:\n",
    "    X_gen, y_gen = augment_training_permutations(\n",
    "        X_gen_base,\n",
    "        y_gen_base,\n",
    "        k=CLASS_TRAIN_PERM_K,\n",
    "        seed=CLASS_PERM_SEED,\n",
    "    )\n",
    "else:\n",
    "    X_gen, y_gen = X_gen_base, y_gen_base\n",
    "\n",
    "encoder_model = load_encoder(MODEL_PATH)\n",
    "Z_gen = encode_latents(encoder_model, X_gen)\n",
    "Z_gen_base = encode_latents(encoder_model, X_gen_base)\n",
    "\n",
    "print(\"Generated base rows:\", len(gen_df))\n",
    "print(\"Generated train rows (after perm aug):\", len(X_gen))\n",
    "print(\"Adobe rows:\", len(adobe_df))\n",
    "print(\"Latent train shape:\", Z_gen.shape)\n",
    "print(\"Latent base shape:\", Z_gen_base.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b94d9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RF search: 24 candidates, 4-fold CV, scoring=accuracy\n",
      "[RF Search] 1/24 | score=0.7819 +/- 0.0043 | iter=12.5s | elapsed=0.2m | eta=4.8m\n",
      "  New best score: 0.7819\n",
      "  Best params: {'n_estimators': 200, 'min_samples_leaf': 1, 'max_features': 0.8, 'max_depth': 12, 'class_weight': 'balanced'}\n",
      "[RF Search] 2/24 | score=0.8049 +/- 0.0033 | iter=30.7s | elapsed=0.7m | eta=7.9m\n",
      "  New best score: 0.8049\n",
      "  Best params: {'n_estimators': 1200, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 12, 'class_weight': 'balanced_subsample'}\n",
      "[RF Search] 3/24 | score=0.8427 +/- 0.0024 | iter=33.0s | elapsed=1.3m | eta=8.9m\n",
      "  New best score: 0.8427\n",
      "  Best params: {'n_estimators': 600, 'min_samples_leaf': 5, 'max_features': 0.5, 'max_depth': None, 'class_weight': 'balanced_subsample'}\n",
      "[RF Search] 4/24 | score=0.8545 +/- 0.0024 | iter=55.3s | elapsed=2.2m | eta=11.0m\n",
      "  New best score: 0.8545\n",
      "  Best params: {'n_estimators': 1200, 'min_samples_leaf': 3, 'max_features': 0.5, 'max_depth': None, 'class_weight': 'balanced'}\n",
      "[RF Search] 5/24 | score=0.7982 +/- 0.0039 | iter=37.4s | elapsed=2.8m | eta=10.7m\n",
      "[RF Search] 6/24 | score=0.8691 +/- 0.0028 | iter=24.0s | elapsed=3.2m | eta=9.6m\n",
      "  New best score: 0.8691\n",
      "  Best params: {'n_estimators': 1000, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': None}\n",
      "[RF Search] 7/24 | score=0.8447 +/- 0.0021 | iter=21.6s | elapsed=3.6m | eta=8.7m\n",
      "[RF Search] 8/24 | score=0.8520 +/- 0.0033 | iter=13.5s | elapsed=3.8m | eta=7.6m\n",
      "[RF Search] 9/24 | score=0.8423 +/- 0.0027 | iter=26.0s | elapsed=4.2m | eta=7.1m\n",
      "[RF Search] 10/24 | score=0.8596 +/- 0.0039 | iter=42.9s | elapsed=4.9m | eta=6.9m\n",
      "[RF Search] 11/24 | score=0.7957 +/- 0.0039 | iter=10.6s | elapsed=5.1m | eta=6.1m\n",
      "[RF Search] 12/24 | score=0.7887 +/- 0.0030 | iter=7.4s | elapsed=5.2m | eta=5.2m\n",
      "[RF Search] 13/24 | score=0.8401 +/- 0.0025 | iter=47.7s | elapsed=6.0m | eta=5.1m\n",
      "[RF Search] 14/24 | score=0.8621 +/- 0.0014 | iter=70.1s | elapsed=7.2m | eta=5.2m\n",
      "[RF Search] 15/24 | score=0.7896 +/- 0.0023 | iter=14.5s | elapsed=7.5m | eta=4.5m\n",
      "[RF Search] 16/24 | score=0.8601 +/- 0.0026 | iter=34.7s | elapsed=8.0m | eta=4.0m\n",
      "[RF Search] 17/24 | score=0.8418 +/- 0.0028 | iter=32.1s | elapsed=8.6m | eta=3.5m\n",
      "[RF Search] 18/24 | score=0.8432 +/- 0.0036 | iter=13.9s | elapsed=8.8m | eta=2.9m\n",
      "[RF Search] 19/24 | score=0.8405 +/- 0.0024 | iter=4.7s | elapsed=8.9m | eta=2.3m\n",
      "[RF Search] 20/24 | score=0.8541 +/- 0.0027 | iter=25.7s | elapsed=9.3m | eta=1.9m\n",
      "[RF Search] 21/24 | score=0.8525 +/- 0.0035 | iter=22.4s | elapsed=9.7m | eta=1.4m\n",
      "[RF Search] 22/24 | score=0.8541 +/- 0.0020 | iter=34.1s | elapsed=10.2m | eta=0.9m\n",
      "[RF Search] 23/24 | score=0.7772 +/- 0.0046 | iter=10.5s | elapsed=10.4m | eta=0.5m\n",
      "[RF Search] 24/24 | score=0.7908 +/- 0.0033 | iter=15.6s | elapsed=10.7m | eta=0.0m\n",
      "RF search finished.\n",
      "Best CV score (accuracy): 0.8691\n",
      "Best RF params: {'n_estimators': 1000, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'class_weight': None}\n",
      "Final RF fit done in 7.9s\n",
      "Calibrating probabilities (method=sigmoid) on held-out calibration split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OHM\\Semester_3\\fast_ai_study\\.venv\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration finished.\n",
      "Validation accuracy: 0.8873\n",
      "Validation top-2 accuracy: 0.9610\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8643    0.9517    0.9059      1800\n",
      "           1     0.8755    0.9217    0.8980      1800\n",
      "           2     0.9747    0.8972    0.9343      1800\n",
      "           3     0.8284    0.7967    0.8122      1800\n",
      "           4     0.8329    0.8111    0.8218      1800\n",
      "           5     0.8936    0.9094    0.9014      1800\n",
      "           6     0.9497    0.9233    0.9363      1800\n",
      "\n",
      "    accuracy                         0.8873     12600\n",
      "   macro avg     0.8884    0.8873    0.8871     12600\n",
      "weighted avg     0.8884    0.8873    0.8871     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Train classifier on generated labels only (auto-tuned RF + optional calibration)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    Z_gen,\n",
    "    y_gen,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=y_gen,\n",
    ")\n",
    "\n",
    "# Split training into fit/calibration sets so validation remains unbiased\n",
    "X_fit, X_cal, y_fit, y_cal = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=RF_CALIBRATION_SPLIT,\n",
    "    random_state=RF_RANDOM_STATE,\n",
    "    stratify=y_train,\n",
    ")\n",
    "\n",
    "rf_best_params = {}\n",
    "rf_best_cv_score = float(\"-inf\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=RF_CV_FOLDS, shuffle=True, random_state=RF_RANDOM_STATE)\n",
    "base_rf = RandomForestClassifier(\n",
    "    random_state=RF_RANDOM_STATE,\n",
    "    n_jobs=RF_FIT_N_JOBS,\n",
    ")\n",
    "\n",
    "if RF_AUTO_TUNE:\n",
    "    sampled_params = list(ParameterSampler(RF_PARAM_GRID, n_iter=RF_SEARCH_N_ITER, random_state=RF_RANDOM_STATE))\n",
    "    total = len(sampled_params)\n",
    "\n",
    "    print(f\"Starting RF search: {total} candidates, {RF_CV_FOLDS}-fold CV, scoring={RF_SCORING}\")\n",
    "    search_start = perf_counter()\n",
    "\n",
    "    for i, params in enumerate(sampled_params, start=1):\n",
    "        iter_start = perf_counter()\n",
    "\n",
    "        model = clone(base_rf)\n",
    "        model.set_params(**params)\n",
    "\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X_fit,\n",
    "            y_fit,\n",
    "            scoring=RF_SCORING,\n",
    "            cv=cv,\n",
    "            n_jobs=RF_CV_N_JOBS,\n",
    "        )\n",
    "\n",
    "        mean_score = float(np.mean(scores))\n",
    "        std_score = float(np.std(scores))\n",
    "\n",
    "        iter_elapsed = perf_counter() - iter_start\n",
    "        elapsed = perf_counter() - search_start\n",
    "        avg_per_iter = elapsed / i\n",
    "        eta = avg_per_iter * (total - i)\n",
    "\n",
    "        print(\n",
    "            f\"[RF Search] {i}/{total} | score={mean_score:.4f} +/- {std_score:.4f} | \"\n",
    "            f\"iter={iter_elapsed:.1f}s | elapsed={elapsed/60:.1f}m | eta={eta/60:.1f}m\"\n",
    "        )\n",
    "\n",
    "        if mean_score > rf_best_cv_score:\n",
    "            rf_best_cv_score = mean_score\n",
    "            rf_best_params = params\n",
    "            print(f\"  New best score: {rf_best_cv_score:.4f}\")\n",
    "            print(f\"  Best params: {rf_best_params}\")\n",
    "\n",
    "    print(\"RF search finished.\")\n",
    "    print(f\"Best CV score ({RF_SCORING}): {rf_best_cv_score:.4f}\")\n",
    "    print(f\"Best RF params: {rf_best_params}\")\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "        **rf_best_params,\n",
    "        random_state=RF_RANDOM_STATE,\n",
    "        n_jobs=RF_FIT_N_JOBS,\n",
    "    )\n",
    "else:\n",
    "    rf_best_params = {k: (v[0] if isinstance(v, list) else v) for k, v in RF_PARAM_GRID.items()}\n",
    "    print(\"RF auto-tune disabled. Using first values from RF_PARAM_GRID:\")\n",
    "    print(rf_best_params)\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "        **rf_best_params,\n",
    "        random_state=RF_RANDOM_STATE,\n",
    "        n_jobs=RF_FIT_N_JOBS,\n",
    "    )\n",
    "\n",
    "fit_start = perf_counter()\n",
    "rf_model.fit(X_fit, y_fit)\n",
    "fit_elapsed = perf_counter() - fit_start\n",
    "print(f\"Final RF fit done in {fit_elapsed:.1f}s\")\n",
    "\n",
    "# Optional probability calibration\n",
    "if RF_USE_PROB_CALIBRATION:\n",
    "    print(f\"Calibrating probabilities (method={RF_CALIBRATION_METHOD}) on held-out calibration split...\")\n",
    "    clf = CalibratedClassifierCV(estimator=rf_model, method=RF_CALIBRATION_METHOD, cv='prefit')\n",
    "    clf.fit(X_cal, y_cal)\n",
    "    print(\"Calibration finished.\")\n",
    "else:\n",
    "    clf = rf_model\n",
    "\n",
    "# Evaluate on untouched validation split\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "valid_acc = float(accuracy_score(y_valid, y_valid_pred))\n",
    "\n",
    "# Optional top-2 diagnostic\n",
    "valid_proba = clf.predict_proba(X_valid)\n",
    "valid_top2 = np.argsort(valid_proba, axis=1)[:, -2:]\n",
    "valid_top2_labels = clf.classes_[valid_top2]\n",
    "valid_top2_acc = float(np.mean([y_valid[i] in valid_top2_labels[i] for i in range(len(y_valid))]))\n",
    "\n",
    "print(f\"Validation accuracy: {valid_acc:.4f}\")\n",
    "print(f\"Validation top-2 accuracy: {valid_top2_acc:.4f}\")\n",
    "print(classification_report(y_valid, y_valid_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8dbea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OHM\\Semester_3\\fast_ai_study\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined UMAP predictions: out\\adobe_umap_law_predictions.csv\n",
      "Saved popularity: out\\adobe_rule_popularity.csv\n",
      "Saved metrics: out\\generated_classifier_metrics.csv\n",
      "Inference permutation passes: 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umap_x</th>\n",
       "      <th>umap_y</th>\n",
       "      <th>umap_z</th>\n",
       "      <th>law</th>\n",
       "      <th>pred_law</th>\n",
       "      <th>pred_confidence</th>\n",
       "      <th>second_law</th>\n",
       "      <th>second_confidence</th>\n",
       "      <th>class_pred_law</th>\n",
       "      <th>class_pred_confidence</th>\n",
       "      <th>class_second_law</th>\n",
       "      <th>class_second_confidence</th>\n",
       "      <th>adobe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.464985</td>\n",
       "      <td>10.679130</td>\n",
       "      <td>2.334833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.249228</td>\n",
       "      <td>10.310564</td>\n",
       "      <td>2.228700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.305106</td>\n",
       "      <td>11.541363</td>\n",
       "      <td>6.234135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.172910</td>\n",
       "      <td>9.486996</td>\n",
       "      <td>2.014287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.005281</td>\n",
       "      <td>9.262683</td>\n",
       "      <td>2.211741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     umap_x     umap_y    umap_z  law  pred_law  pred_confidence  second_law  \\\n",
       "0  4.464985  10.679130  2.334833    0         0              1.0          -1   \n",
       "1  3.249228  10.310564  2.228700    0         0              1.0          -1   \n",
       "2  2.305106  11.541363  6.234135    0         0              1.0          -1   \n",
       "3  3.172910   9.486996  2.014287    0         0              1.0          -1   \n",
       "4  4.005281   9.262683  2.211741    0         0              1.0          -1   \n",
       "\n",
       "   second_confidence  class_pred_law  class_pred_confidence  class_second_law  \\\n",
       "0                0.0               0                    1.0                -1   \n",
       "1                0.0               0                    1.0                -1   \n",
       "2                0.0               0                    1.0                -1   \n",
       "3                0.0               0                    1.0                -1   \n",
       "4                0.0               0                    1.0                -1   \n",
       "\n",
       "   class_second_confidence  adobe  \n",
       "0                      0.0      0  \n",
       "1                      0.0      0  \n",
       "2                      0.0      0  \n",
       "3                      0.0      0  \n",
       "4                      0.0      0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Predict Adobe rules + UMAP + combined output CSV\n",
    "\n",
    "law_ids = np.array(sorted(np.unique(y_gen)))\n",
    "centroids = np.stack([Z_gen[y_gen == law].mean(axis=0) for law in law_ids], axis=0)\n",
    "c_norm = centroids / np.linalg.norm(centroids, axis=1, keepdims=True)\n",
    "\n",
    "# Build permutation list for inference averaging\n",
    "if CLASS_INFER_PERM_AVG:\n",
    "    rng = np.random.default_rng(CLASS_PERM_SEED)\n",
    "    infer_perms = [np.arange(COLOR_COUNT)] + [rng.permutation(COLOR_COUNT) for _ in range(CLASS_INFER_PERM_K)]\n",
    "else:\n",
    "    infer_perms = [np.arange(COLOR_COUNT)]\n",
    "\n",
    "class_proba_sum = None\n",
    "proto_proba_sum = None\n",
    "Z_adobe_sum = None\n",
    "\n",
    "for perm in infer_perms:\n",
    "    Xp = apply_palette_permutation(X_adobe_base, perm)\n",
    "    Zp = encode_latents(encoder_model, Xp)\n",
    "\n",
    "    # For UMAP and optional downstream use\n",
    "    Z_adobe_sum = Zp if Z_adobe_sum is None else (Z_adobe_sum + Zp)\n",
    "\n",
    "    # -------- Classifier-based probabilities --------\n",
    "    class_proba = clf.predict_proba(Zp)\n",
    "    class_proba_sum = class_proba if class_proba_sum is None else (class_proba_sum + class_proba)\n",
    "\n",
    "    # -------- Prototype-based probabilities --------\n",
    "    z_norm = Zp / np.linalg.norm(Zp, axis=1, keepdims=True)\n",
    "    cos_sim = z_norm @ c_norm.T\n",
    "    cos_dist = 1.0 - cos_sim\n",
    "    proto_proba = softmax_from_neg_dist(cos_dist, scale=PROTO_CONF_SCALE)\n",
    "    proto_proba_sum = proto_proba if proto_proba_sum is None else (proto_proba_sum + proto_proba)\n",
    "\n",
    "num_passes = len(infer_perms)\n",
    "class_proba_avg = class_proba_sum / num_passes\n",
    "proto_proba_avg = proto_proba_sum / num_passes\n",
    "Z_adobe = Z_adobe_sum / num_passes\n",
    "\n",
    "# -------- Classifier-based predictions (prefixed with class_) --------\n",
    "class_ids = clf.classes_\n",
    "class_top2_idx = np.argsort(class_proba_avg, axis=1)[:, -2:]\n",
    "class_second_idx = class_top2_idx[:, 0]\n",
    "class_first_idx = class_top2_idx[:, 1]\n",
    "\n",
    "class_pred_law = class_ids[class_first_idx]\n",
    "class_pred_confidence = class_proba_avg[np.arange(len(class_proba_avg)), class_first_idx]\n",
    "class_second_law = class_ids[class_second_idx]\n",
    "class_second_confidence = class_proba_avg[np.arange(len(class_proba_avg)), class_second_idx]\n",
    "\n",
    "# -------- Prototype predictions (clean names, no prefix) --------\n",
    "proto_top2_idx = np.argsort(proto_proba_avg, axis=1)[:, -2:]\n",
    "proto_second_idx = proto_top2_idx[:, 0]\n",
    "proto_first_idx = proto_top2_idx[:, 1]\n",
    "\n",
    "pred_law = law_ids[proto_first_idx]\n",
    "pred_confidence = proto_proba_avg[np.arange(len(proto_proba_avg)), proto_first_idx]\n",
    "second_law = law_ids[proto_second_idx]\n",
    "second_confidence = proto_proba_avg[np.arange(len(proto_proba_avg)), proto_second_idx]\n",
    "\n",
    "# Build a combined latent set for UMAP context\n",
    "if INCLUDE_GENERATED_IN_UMAP:\n",
    "    Z_all = np.vstack([Z_gen_base, Z_adobe])\n",
    "else:\n",
    "    Z_all = Z_adobe\n",
    "\n",
    "umap_3d = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.05, metric=\"euclidean\", random_state=42)\n",
    "Z_all_3d = umap_3d.fit_transform(Z_all)\n",
    "\n",
    "if INCLUDE_GENERATED_IN_UMAP:\n",
    "    n_gen = len(Z_gen_base)\n",
    "    gen_umap = Z_all_3d[:n_gen]\n",
    "    adobe_umap = Z_all_3d[n_gen:]\n",
    "\n",
    "    # Generated rows (for context), keep known law labels\n",
    "    gen_out = pd.DataFrame({\n",
    "        \"umap_x\": gen_umap[:, 0],\n",
    "        \"umap_y\": gen_umap[:, 1],\n",
    "        \"umap_z\": gen_umap[:, 2],\n",
    "        \"law\": y_gen_base,\n",
    "        \"pred_law\": y_gen_base,\n",
    "        \"pred_confidence\": np.ones(n_gen, dtype=\"float32\"),\n",
    "        \"second_law\": np.full(n_gen, -1, dtype=\"int64\"),\n",
    "        \"second_confidence\": np.zeros(n_gen, dtype=\"float32\"),\n",
    "        \"class_pred_law\": y_gen_base,\n",
    "        \"class_pred_confidence\": np.ones(n_gen, dtype=\"float32\"),\n",
    "        \"class_second_law\": np.full(n_gen, -1, dtype=\"int64\"),\n",
    "        \"class_second_confidence\": np.zeros(n_gen, dtype=\"float32\"),\n",
    "        \"adobe\": np.zeros(n_gen, dtype=\"int64\"),\n",
    "    })\n",
    "\n",
    "    # Adobe rows with predictions\n",
    "    adobe_out = pd.DataFrame({\n",
    "        \"umap_x\": adobe_umap[:, 0],\n",
    "        \"umap_y\": adobe_umap[:, 1],\n",
    "        \"umap_z\": adobe_umap[:, 2],\n",
    "        \"law\": np.full(len(adobe_df), -1, dtype=\"int64\"),\n",
    "        \"pred_law\": pred_law,\n",
    "        \"pred_confidence\": pred_confidence,\n",
    "        \"second_law\": second_law,\n",
    "        \"second_confidence\": second_confidence,\n",
    "        \"class_pred_law\": class_pred_law,\n",
    "        \"class_pred_confidence\": class_pred_confidence,\n",
    "        \"class_second_law\": class_second_law,\n",
    "        \"class_second_confidence\": class_second_confidence,\n",
    "        \"adobe\": np.ones(len(adobe_df), dtype=\"int64\"),\n",
    "    })\n",
    "\n",
    "    out_df = pd.concat([gen_out, adobe_out], ignore_index=True)\n",
    "else:\n",
    "    # Adobe-only output mode\n",
    "    out_df = pd.DataFrame({\n",
    "        \"umap_x\": Z_all_3d[:, 0],\n",
    "        \"umap_y\": Z_all_3d[:, 1],\n",
    "        \"umap_z\": Z_all_3d[:, 2],\n",
    "        \"law\": np.full(len(adobe_df), -1, dtype=\"int64\"),\n",
    "        \"pred_law\": pred_law,\n",
    "        \"pred_confidence\": pred_confidence,\n",
    "        \"second_law\": second_law,\n",
    "        \"second_confidence\": second_confidence,\n",
    "        \"class_pred_law\": class_pred_law,\n",
    "        \"class_pred_confidence\": class_pred_confidence,\n",
    "        \"class_second_law\": class_second_law,\n",
    "        \"class_second_confidence\": class_second_confidence,\n",
    "        \"adobe\": np.ones(len(adobe_df), dtype=\"int64\"),\n",
    "    })\n",
    "\n",
    "# Popularity summary from Adobe-only clean (prototype) predictions\n",
    "popularity_df = (\n",
    "    pd.Series(pred_law)\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis(\"law\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "popularity_df[\"share\"] = popularity_df[\"count\"] / popularity_df[\"count\"].sum()\n",
    "\n",
    "OUT_UMAP_PRED_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_df.to_csv(OUT_UMAP_PRED_PATH, index=False)\n",
    "popularity_df.to_csv(OUT_POPULARITY_PATH, index=False)\n",
    "\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        \"validation_accuracy\": valid_acc,\n",
    "        \"validation_top2_accuracy\": valid_top2_acc,\n",
    "        \"classifier_type\": \"RandomForestClassifier\",\n",
    "        \"rf_auto_tune\": RF_AUTO_TUNE,\n",
    "        \"rf_scoring\": RF_SCORING,\n",
    "        \"rf_search_n_iter\": RF_SEARCH_N_ITER,\n",
    "        \"rf_cv_folds\": RF_CV_FOLDS,\n",
    "        \"rf_best_cv_score\": rf_best_cv_score,\n",
    "        \"rf_best_params\": str(rf_best_params),\n",
    "        \"rf_param_grid\": str(RF_PARAM_GRID),\n",
    "        \"rf_fit_n_jobs\": RF_FIT_N_JOBS,\n",
    "        \"rf_cv_n_jobs\": RF_CV_N_JOBS,\n",
    "        \"rf_use_prob_calibration\": RF_USE_PROB_CALIBRATION,\n",
    "        \"rf_calibration_method\": RF_CALIBRATION_METHOD,\n",
    "        \"rf_calibration_split\": RF_CALIBRATION_SPLIT,\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"generated_path\": str(generated_path),\n",
    "        \"model_path\": str(MODEL_PATH),\n",
    "        \"adobe_path\": str(adobe_path),\n",
    "        \"train_perm_augment\": CLASS_TRAIN_PERM_AUGMENT,\n",
    "        \"train_perm_k\": CLASS_TRAIN_PERM_K,\n",
    "        \"infer_perm_avg\": CLASS_INFER_PERM_AVG,\n",
    "        \"infer_perm_k\": CLASS_INFER_PERM_K,\n",
    "        \"perm_seed\": CLASS_PERM_SEED,\n",
    "        \"perm_passes_used\": num_passes,\n",
    "        \"include_generated_in_umap\": INCLUDE_GENERATED_IN_UMAP,\n",
    "        \"generated_rows_in_output\": int(len(gen_df) if INCLUDE_GENERATED_IN_UMAP else 0),\n",
    "        \"adobe_rows_in_output\": int(len(adobe_df)),\n",
    "    }\n",
    "])\n",
    "metrics_df.to_csv(OUT_METRICS_PATH, index=False)\n",
    "\n",
    "print(\"Saved combined UMAP predictions:\", OUT_UMAP_PRED_PATH)\n",
    "print(\"Saved popularity:\", OUT_POPULARITY_PATH)\n",
    "print(\"Saved metrics:\", OUT_METRICS_PATH)\n",
    "print(f\"Inference permutation passes: {num_passes}\")\n",
    "out_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
